{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm7ezOuMVgpi"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers datasets evaluate accelerate\n",
        "!pip install scikit-learn\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "import evaluate\n",
        "import glob\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4N0iUV9oVrkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_PROCS = 32\n",
        "LR = 1e-4\n",
        "EPOCHS = 10\n",
        "MODEL = 'bert-base-uncased'\n",
        "OUT_DIR = 'arxiv_bert'\n",
        "PATH = {\n",
        "    \"train\":'NLP2.csv',\n",
        "    \"test\":'test.csv',\n",
        "    \"validation\":'validation.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "EPx86EyLWRBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: how do i rename dataset columsn\n",
        "\n",
        "# dataset = load_dataset(PATH, split='train')\n",
        "# dataset = dataset.rename_column(\"text\", \"sentence1\")\n",
        "# dataset = dataset.rename_column(\"label\", \"labels\")\n"
      ],
      "metadata": {
        "id": "yskbTCn0X1SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"/content\",data_files=PATH, split='train')\n",
        "valid_dataset = load_dataset(\"/content\",data_files=PATH, split='validation')\n",
        "test_dataset = load_dataset(\"/content\",data_files=PATH, split='test')\n",
        "# dataset = load_dataset(PATH, split='train')\n",
        "train_dataset = train_dataset.rename_column(\"Text\", \"text\")\n",
        "train_dataset = train_dataset.rename_column(\"Labels\", \"label\")\n",
        "valid_dataset = valid_dataset.rename_column(\"Text\", \"text\")\n",
        "valid_dataset = valid_dataset.rename_column(\"Labels\", \"label\")\n",
        "test_dataset = test_dataset.rename_column(\"Text\", \"text\")\n",
        "test_dataset = test_dataset.rename_column(\"Labels\", \"label\")\n",
        "print(train_dataset)\n",
        "print(valid_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "T5tFkqTBhK9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "729uibKaVxxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "vRgWWGKAV10M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # print(examples)\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "yEsPEhvGV3jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_proc=NUM_PROCS\n",
        ")\n",
        "\n",
        "tokenized_valid = valid_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_proc=NUM_PROCS\n",
        ")\n",
        "\n",
        "tokenized_test = test_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_proc=NUM_PROCS\n",
        ")"
      ],
      "metadata": {
        "id": "Tx-8ICSyV5SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "0J9lBnMLV7O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sample = preprocess_function(train_dataset[0])\n",
        "print(tokenized_sample)\n",
        "print(f\"Length of tokenized IDs: {len(tokenized_sample.input_ids)}\")\n",
        "print(f\"Length of attention mask: {len(tokenized_sample.attention_mask)}\")"
      ],
      "metadata": {
        "id": "Nsml0NX8V9NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sample = preprocess_function(train_dataset[0])\n",
        "print(tokenized_sample)"
      ],
      "metadata": {
        "id": "rNr5R90YV-1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load('accuracy')\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "Ervk70O9WCNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels=2,\n",
        ")"
      ],
      "metadata": {
        "id": "vz0hPhrTWDxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    learning_rate=LR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.001,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=3,\n",
        "    report_to='tensorboard',\n",
        "    fp16=True\n",
        ")"
      ],
      "metadata": {
        "id": "UIylXTsWWHC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "history = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "WcYqAkLgWIwj",
        "outputId": "6d0d2acb-e0ad-43a3-d18b-de08adcb3d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 01:14, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.775742</td>\n",
              "      <td>0.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.717178</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.763219</td>\n",
              "      <td>0.529412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.885006</td>\n",
              "      <td>0.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.831539</td>\n",
              "      <td>0.441176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.846514</td>\n",
              "      <td>0.441176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.889738</td>\n",
              "      <td>0.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.938234</td>\n",
              "      <td>0.352941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.955523</td>\n",
              "      <td>0.382353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.957710</td>\n",
              "      <td>0.411765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(tokenized_test)"
      ],
      "metadata": {
        "id": "l4lMTopYWLUX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "3ae0a8e4-887c-4652-afb2-a7ead6e51139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6953545808792114,\n",
              " 'eval_accuracy': 0.5172413793103449,\n",
              " 'eval_runtime': 0.0346,\n",
              " 'eval_samples_per_second': 837.463,\n",
              " 'eval_steps_per_second': 28.878,\n",
              " 'epoch': 10.0}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AutoModelForSequenceClassification.from_pretrained(f\"arxiv_bert/checkpoint-4440\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "# classify = pipeline(task='text-classification', model=model, tokenizer=tokenizer)\n",
        "\n",
        "# all_files = glob.glob('inference_data/*')\n",
        "# for file_name in all_files:\n",
        "#     file = open(file_name)\n",
        "#     content = file.read()\n",
        "#     print(content)\n",
        "#     result = classify(content)\n",
        "#     print('PRED: ', result)\n",
        "#     print('GT: ', file_name.split('_')[-1].split('.txt')[0])\n",
        "#     print('\\n')"
      ],
      "metadata": {
        "id": "2CAvw3XvWOYZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}